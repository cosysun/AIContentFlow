# AIçƒ­ç‚¹ç›‘æ§ç³»ç»Ÿ

> ç‰ˆæœ¬ï¼šv1.0  
> æ›´æ–°æ—¶é—´ï¼š2026-02-21  
> ç”¨é€”ï¼šè‡ªåŠ¨è¿½è¸ªAIè¡Œä¸šåŠ¨æ€ï¼Œæ¯æ—¥æ¨é€å€™é€‰ä¸»é¢˜

---

## ğŸ“‹ ç›®å½•

1. [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
2. [ç›‘æ§æºé…ç½®](#ç›‘æ§æºé…ç½®)
3. [è‡ªåŠ¨åŒ–è„šæœ¬](#è‡ªåŠ¨åŒ–è„šæœ¬)
4. [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
5. [è¾“å‡ºç¤ºä¾‹](#è¾“å‡ºç¤ºä¾‹)

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   æ•°æ®é‡‡é›†å±‚                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Google News API  â”‚  HackerNews  â”‚  Reddit API      â”‚
â”‚  Twitter API      â”‚  ProductHunt â”‚  arXiv RSS       â”‚
â”‚  TechCrunch RSS   â”‚  Brave Searchâ”‚  GitHub Trending â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   æ•°æ®å¤„ç†å±‚                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ å…³é”®è¯è¿‡æ»¤                                         â”‚
â”‚  â€¢ çƒ­åº¦è®¡ç®—ï¼ˆè®¨è®ºé‡ã€æœç´¢é‡ã€ä¼ æ’­é€Ÿåº¦ï¼‰                â”‚
â”‚  â€¢ å»é‡                                              â”‚
â”‚  â€¢ åˆ†ç±»ï¼ˆAIç§‘æ™®/AIå·¥å…·/AIç¼–ç¨‹/AIå‡ºæµ·åˆ›ä¸šï¼‰            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   è¯„åˆ†å±‚                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  è°ƒç”¨ topic_scorer.py                                â”‚
â”‚  â€¢ è‡ªåŠ¨åˆæ­¥è¯„åˆ†                                       â”‚
â”‚  â€¢ æ¨èä¼˜å…ˆçº§æ’åº                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   è¾“å‡ºå±‚                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ MarkdownæŠ¥å‘Š                                      â”‚
â”‚  â€¢ JSONæ•°æ®                                          â”‚
â”‚  â€¢ é‚®ä»¶/ä¼ä¸šå¾®ä¿¡æ¨é€ï¼ˆå¯é€‰ï¼‰                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¡ ç›‘æ§æºé…ç½®

### 1. Google Newsï¼ˆéœ€API Keyï¼‰

**ç”¨é€”**ï¼šè·å–ä¸»æµåª’ä½“æŠ¥é“  
**æ›´æ–°é¢‘ç‡**ï¼šå®æ—¶  
**å…³é”®è¯**ï¼š
```
AI, Artificial Intelligence, Machine Learning, 
Deep Learning, GPT, Gemini, Claude, LLM, 
Generative AI, AI Tools, AI Coding
```

**APIé…ç½®**ï¼š
```python
GOOGLE_NEWS_CONFIG = {
    "api_key": "YOUR_API_KEY",  # ä» https://newsapi.org è·å–
    "keywords": ["AI", "Artificial Intelligence", "Machine Learning"],
    "language": "en",
    "sortBy": "publishedAt",
    "pageSize": 50
}
```

---

### 2. HackerNewsï¼ˆæ— éœ€API Keyï¼‰

**ç”¨é€”**ï¼šå¼€å‘è€…ç¤¾åŒºçƒ­è®®è¯é¢˜  
**æ›´æ–°é¢‘ç‡**ï¼šæ¯å°æ—¶  
**æŠ“å–å†…å®¹**ï¼šTop Storieså‰30æ¡

**APIç«¯ç‚¹**ï¼š
```
https://hacker-news.firebaseio.com/v0/topstories.json
```

**ä»£ç ç¤ºä¾‹**ï¼š
```python
import requests

def fetch_hackernews_top():
    """è·å–HackerNewsçƒ­é—¨è¯é¢˜"""
    # è·å–Top Stories IDåˆ—è¡¨
    url = "https://hacker-news.firebaseio.com/v0/topstories.json"
    story_ids = requests.get(url).json()[:30]
    
    stories = []
    for story_id in story_ids:
        story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        story = requests.get(story_url).json()
        
        # è¿‡æ»¤AIç›¸å…³
        if story and 'title' in story:
            title = story['title'].lower()
            if any(kw in title for kw in ['ai', 'gpt', 'llm', 'machine learning']):
                stories.append({
                    "title": story['title'],
                    "url": story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                    "score": story.get('score', 0),
                    "comments": story.get('descendants', 0)
                })
    
    return stories
```

---

### 3. Redditï¼ˆéœ€APIè®¤è¯ï¼‰

**ç”¨é€”**ï¼šæŠ€æœ¯ç¤¾åŒºæ·±åº¦è®¨è®º  
**ç›‘æ§å­ç‰ˆå—**ï¼š
- r/MachineLearning
- r/artificial
- r/ArtificialIntelligence
- r/LocalLLaMA
- r/OpenAI

**APIé…ç½®**ï¼š
```python
REDDIT_CONFIG = {
    "client_id": "YOUR_CLIENT_ID",
    "client_secret": "YOUR_CLIENT_SECRET",
    "user_agent": "AI Topic Monitor v1.0"
}

# éœ€è¦åœ¨ https://www.reddit.com/prefs/apps åˆ›å»ºåº”ç”¨
```

---

### 4. Twitter/Xï¼ˆéœ€API v2è®¤è¯ï¼‰

**ç”¨é€”**ï¼šå®æ—¶çƒ­ç‚¹è¿½è¸ª  
**ç›‘æ§è´¦å·**ï¼ˆAI KOLï¼‰ï¼š
```
@sama (Sam Altman - OpenAI)
@drfeifei (Fei-Fei Li - Stanford)
@AndrewYNg (Andrew Ng)
@karpathy (Andrej Karpathy)
@emollick (Ethan Mollick - AIæ•™è‚²)
@ylecun (Yann LeCun - Meta)
@GaryMarcus (Gary Marcus - AIæ‰¹è¯„è€…)
```

**APIé…ç½®**ï¼š
```python
TWITTER_CONFIG = {
    "bearer_token": "YOUR_BEARER_TOKEN",
    "tracked_users": [
        "sama", "drfeifei", "AndrewYNg", "karpathy",
        "emollick", "ylecun", "GaryMarcus"
    ],
    "keywords": ["AI", "GPT", "LLM", "Gemini"]
}
```

---

### 5. ProductHuntï¼ˆæ— éœ€API Keyï¼‰

**ç”¨é€”**ï¼šæ–°AIå·¥å…·å‘ç°  
**æŠ“å–å†…å®¹**ï¼šæ¯æ—¥Top 10 AIå·¥å…·

**RSSè®¢é˜…**ï¼š
```
https://www.producthunt.com/topics/artificial-intelligence.rss
```

**ä»£ç ç¤ºä¾‹**ï¼š
```python
import feedparser

def fetch_producthunt_ai_tools():
    """è·å–ProductHunt AIå·¥å…·"""
    feed = feedparser.parse("https://www.producthunt.com/topics/artificial-intelligence.rss")
    
    tools = []
    for entry in feed.entries[:10]:
        tools.append({
            "name": entry.title,
            "url": entry.link,
            "description": entry.summary,
            "published": entry.published
        })
    
    return tools
```

---

### 6. arXivï¼ˆæ— éœ€API Keyï¼‰

**ç”¨é€”**ï¼šå­¦æœ¯å‰æ²¿è®ºæ–‡  
**ç›‘æ§åˆ†ç±»**ï¼š
- cs.AIï¼ˆäººå·¥æ™ºèƒ½ï¼‰
- cs.CLï¼ˆè®¡ç®—è¯­è¨€å­¦ï¼‰
- cs.CVï¼ˆè®¡ç®—æœºè§†è§‰ï¼‰
- cs.LGï¼ˆæœºå™¨å­¦ä¹ ï¼‰

**RSSè®¢é˜…**ï¼š
```
http://export.arxiv.org/rss/cs.AI
http://export.arxiv.org/rss/cs.CL
http://export.arxiv.org/rss/cs.LG
```

---

### 7. TechCrunchï¼ˆæ— éœ€API Keyï¼‰

**ç”¨é€”**ï¼šç§‘æŠ€æ–°é—»æŠ¥é“  
**RSSè®¢é˜…**ï¼š
```
https://techcrunch.com/category/artificial-intelligence/feed/
```

---

### 8. Brave Search APIï¼ˆæ¨èï¼Œæ›¿ä»£Googleï¼‰

**ç”¨é€”**ï¼šæœç´¢è¶‹åŠ¿åˆ†æ  
**ä¼˜åŠ¿**ï¼šå…è´¹é¢åº¦æ›´é«˜ï¼Œæ— éœ€ç¿»å¢™

**APIé…ç½®**ï¼š
```python
BRAVE_SEARCH_CONFIG = {
    "api_key": "YOUR_BRAVE_API_KEY",  # ä» https://brave.com/search/api/ è·å–
    "endpoint": "https://api.search.brave.com/res/v1/web/search",
    "queries": [
        "AI news today",
        "new AI tools",
        "AI startup funding",
        "latest AI research"
    ]
}
```

---

### 9. GitHub Trendingï¼ˆæ— éœ€API Keyï¼‰

**ç”¨é€”**ï¼šçƒ­é—¨AIé¡¹ç›®  
**ç›‘æ§è¯­è¨€**ï¼šPython, JavaScript, TypeScript

**æŠ“å–URL**ï¼š
```
https://github.com/trending/python?since=daily
https://github.com/trending/jupyter-notebook?since=daily
```

---

## ğŸ¤– è‡ªåŠ¨åŒ–è„šæœ¬

### ä¸»è„šæœ¬ï¼šai_monitor.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIçƒ­ç‚¹ç›‘æ§ç³»ç»Ÿ
æ¯æ—¥è‡ªåŠ¨æŠ“å–AIè¡Œä¸šåŠ¨æ€ï¼Œç”Ÿæˆå€™é€‰ä¸»é¢˜æŠ¥å‘Š

è¿è¡Œæ–¹å¼ï¼š
    python ai_monitor.py              # è¿è¡Œä¸€æ¬¡
    python ai_monitor.py --daemon     # åå°è¿è¡Œï¼ˆæ¯4å°æ—¶ï¼‰
    python ai_monitor.py --test       # æµ‹è¯•æ¨¡å¼
"""

import requests
import feedparser
import json
from datetime import datetime, timedelta
from collections import Counter
from typing import List, Dict
import time

# ============================================
# é…ç½®
# ============================================

# Brave Search APIé…ç½®ï¼ˆæ¨èï¼‰
BRAVE_API_KEY = "YOUR_API_KEY"  # ä» https://brave.com/search/api/ è·å–

# å¯é€‰ï¼šå…¶ä»–APIé…ç½®
REDDIT_CLIENT_ID = ""
REDDIT_CLIENT_SECRET = ""
TWITTER_BEARER_TOKEN = ""

# å…³é”®è¯é…ç½®
AI_KEYWORDS = [
    "AI", "artificial intelligence", "machine learning", "deep learning",
    "GPT", "Gemini", "Claude", "LLM", "large language model",
    "generative AI", "AI tools", "AI coding", "AI startup"
]

# çƒ­åº¦é˜ˆå€¼
TRENDING_THRESHOLD = {
    "high": 5000,    # 24å°æ—¶å†…è®¨è®ºé‡ > 5000
    "medium": 1000,  # 1000-5000
    "low": 100       # 100-1000
}

# ============================================
# æ•°æ®é‡‡é›†æ¨¡å—
# ============================================

def fetch_hackernews():
    """æŠ“å–HackerNewsçƒ­é—¨è¯é¢˜"""
    print("ğŸ“° æ­£åœ¨æŠ“å– HackerNews...")
    
    try:
        url = "https://hacker-news.firebaseio.com/v0/topstories.json"
        story_ids = requests.get(url, timeout=10).json()[:30]
        
        stories = []
        for story_id in story_ids:
            story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            story = requests.get(story_url, timeout=10).json()
            
            if story and 'title' in story:
                title = story['title'].lower()
                if any(kw.lower() in title for kw in AI_KEYWORDS):
                    stories.append({
                        "source": "HackerNews",
                        "title": story['title'],
                        "url": story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                        "score": story.get('score', 0),
                        "comments": story.get('descendants', 0),
                        "timestamp": datetime.now().isoformat()
                    })
        
        print(f"  âœ… æ‰¾åˆ° {len(stories)} æ¡AIç›¸å…³è¯é¢˜")
        return stories
    
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥ï¼š{e}")
        return []

def fetch_brave_search():
    """ä½¿ç”¨Brave Search APIæœç´¢AIçƒ­ç‚¹"""
    print("ğŸ” æ­£åœ¨ä½¿ç”¨ Brave Search...")
    
    if not BRAVE_API_KEY or BRAVE_API_KEY == "YOUR_API_KEY":
        print("  âš ï¸  æœªé…ç½®Brave API Keyï¼Œè·³è¿‡")
        return []
    
    try:
        headers = {
            "Accept": "application/json",
            "X-Subscription-Token": BRAVE_API_KEY
        }
        
        results = []
        queries = ["AI news today", "new AI tools 2026", "AI startup"]
        
        for query in queries:
            url = f"https://api.search.brave.com/res/v1/web/search?q={query}&count=10"
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                for item in data.get('web', {}).get('results', []):
                    results.append({
                        "source": "Brave Search",
                        "title": item.get('title'),
                        "url": item.get('url'),
                        "description": item.get('description'),
                        "timestamp": datetime.now().isoformat()
                    })
        
        print(f"  âœ… æ‰¾åˆ° {len(results)} æ¡æœç´¢ç»“æœ")
        return results
    
    except Exception as e:
        print(f"  âŒ æœç´¢å¤±è´¥ï¼š{e}")
        return []

def fetch_producthunt():
    """æŠ“å–ProductHunt AIå·¥å…·"""
    print("ğŸš€ æ­£åœ¨æŠ“å– ProductHunt...")
    
    try:
        feed = feedparser.parse("https://www.producthunt.com/topics/artificial-intelligence.rss")
        
        tools = []
        for entry in feed.entries[:10]:
            tools.append({
                "source": "ProductHunt",
                "title": entry.title,
                "url": entry.link,
                "description": entry.get('summary', ''),
                "timestamp": entry.get('published', datetime.now().isoformat())
            })
        
        print(f"  âœ… æ‰¾åˆ° {len(tools)} ä¸ªAIå·¥å…·")
        return tools
    
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥ï¼š{e}")
        return []

def fetch_arxiv():
    """æŠ“å–arXiv AIè®ºæ–‡"""
    print("ğŸ“š æ­£åœ¨æŠ“å– arXiv...")
    
    try:
        feeds = [
            "http://export.arxiv.org/rss/cs.AI",
            "http://export.arxiv.org/rss/cs.CL",
            "http://export.arxiv.org/rss/cs.LG"
        ]
        
        papers = []
        for feed_url in feeds:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries[:5]:
                papers.append({
                    "source": "arXiv",
                    "title": entry.title,
                    "url": entry.link,
                    "description": entry.get('summary', ''),
                    "timestamp": entry.get('published', datetime.now().isoformat())
                })
        
        print(f"  âœ… æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥ï¼š{e}")
        return []

def fetch_techcrunch():
    """æŠ“å–TechCrunch AIæ–°é—»"""
    print("ğŸ“° æ­£åœ¨æŠ“å– TechCrunch...")
    
    try:
        feed = feedparser.parse("https://techcrunch.com/category/artificial-intelligence/feed/")
        
        news = []
        for entry in feed.entries[:10]:
            news.append({
                "source": "TechCrunch",
                "title": entry.title,
                "url": entry.link,
                "description": entry.get('summary', ''),
                "timestamp": entry.get('published', datetime.now().isoformat())
            })
        
        print(f"  âœ… æ‰¾åˆ° {len(news)} æ¡æ–°é—»")
        return news
    
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥ï¼š{e}")
        return []

# ============================================
# æ•°æ®å¤„ç†æ¨¡å—
# ============================================

def calculate_heat(item: Dict) -> int:
    """è®¡ç®—çƒ­åº¦åˆ†æ•°"""
    heat = 0
    
    # HackerNewsçƒ­åº¦
    if item['source'] == 'HackerNews':
        heat = item.get('score', 0) + item.get('comments', 0) * 2
    
    # å…¶ä»–æ¥æºåŸºç¡€åˆ†
    else:
        heat = 100
    
    # æ—¶æ•ˆæ€§åŠ æˆ
    try:
        timestamp = datetime.fromisoformat(item['timestamp'].replace('Z', '+00:00'))
        hours_ago = (datetime.now() - timestamp.replace(tzinfo=None)).total_seconds() / 3600
        if hours_ago < 24:
            heat *= 2
        elif hours_ago < 72:
            heat *= 1.5
    except:
        pass
    
    return int(heat)

def classify_topic(title: str, description: str = "") -> List[str]:
    """åˆ†ç±»åˆ°å†…å®¹çº¿"""
    text = (title + " " + description).lower()
    
    categories = []
    
    # AIç§‘æ™®å…³é”®è¯
    if any(kw in text for kw in ['explain', 'understand', 'what is', 'how does', 'introduction']):
        categories.append("AIç§‘æ™®")
    
    # AIå·¥å…·å…³é”®è¯
    if any(kw in text for kw in ['tool', 'app', 'software', 'platform', 'api', 'product']):
        categories.append("AIå·¥å…·")
    
    # AIç¼–ç¨‹å…³é”®è¯
    if any(kw in text for kw in ['code', 'coding', 'programming', 'developer', 'github', 'open source']):
        categories.append("AIç¼–ç¨‹")
    
    # AIå‡ºæµ·åˆ›ä¸šå…³é”®è¯
    if any(kw in text for kw in ['startup', 'funding', 'market', 'business', 'revenue', 'valuation']):
        categories.append("AIå‡ºæµ·åˆ›ä¸š")
    
    # é»˜è®¤åˆ†ç±»
    if not categories:
        categories.append("AIç§‘æ™®")
    
    return categories

def deduplicate(items: List[Dict]) -> List[Dict]:
    """å»é‡"""
    seen_titles = set()
    unique_items = []
    
    for item in items:
        title_lower = item['title'].lower()
        if title_lower not in seen_titles:
            seen_titles.add(title_lower)
            unique_items.append(item)
    
    return unique_items

# ============================================
# æŠ¥å‘Šç”Ÿæˆæ¨¡å—
# ============================================

def generate_report(items: List[Dict]) -> str:
    """ç”ŸæˆMarkdownæŠ¥å‘Š"""
    
    # æŒ‰çƒ­åº¦æ’åº
    items_with_heat = []
    for item in items:
        item['heat'] = calculate_heat(item)
        item['categories'] = classify_topic(item['title'], item.get('description', ''))
        items_with_heat.append(item)
    
    items_sorted = sorted(items_with_heat, key=lambda x: x['heat'], reverse=True)
    
    # åˆ†çº§
    high_heat = [x for x in items_sorted if x['heat'] >= TRENDING_THRESHOLD['high']]
    medium_heat = [x for x in items_sorted if TRENDING_THRESHOLD['medium'] <= x['heat'] < TRENDING_THRESHOLD['high']]
    low_heat = [x for x in items_sorted if TRENDING_THRESHOLD['low'] <= x['heat'] < TRENDING_THRESHOLD['medium']]
    
    # ç”ŸæˆæŠ¥å‘Š
    report = []
    report.append(f"# AIçƒ­ç‚¹æ—¥æŠ¥")
    report.append(f"")
    report.append(f"> ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"> æ•°æ®æºï¼šHackerNews, ProductHunt, arXiv, TechCrunch, Brave Search")
    report.append(f"> æ€»è®¡ï¼š{len(items_sorted)} æ¡çƒ­ç‚¹")
    report.append(f"")
    report.append(f"---")
    report.append(f"")
    
    # é«˜çƒ­åº¦
    report.append(f"## ğŸ”¥ğŸ”¥ğŸ”¥ é«˜çƒ­åº¦ï¼ˆ{len(high_heat)}æ¡ï¼‰")
    report.append(f"")
    for i, item in enumerate(high_heat, 1):
        report.append(f"### {i}. {item['title']}")
        report.append(f"")
        report.append(f"- **æ¥æº**ï¼š{item['source']}")
        report.append(f"- **çƒ­åº¦**ï¼š{item['heat']}")
        report.append(f"- **æ¨èå†…å®¹çº¿**ï¼š{', '.join(item['categories'])}")
        report.append(f"- **é“¾æ¥**ï¼š{item['url']}")
        if item.get('description'):
            report.append(f"- **ç®€ä»‹**ï¼š{item['description'][:200]}...")
        report.append(f"")
    
    # ä¸­çƒ­åº¦
    if medium_heat:
        report.append(f"## ğŸ”¥ ä¸­çƒ­åº¦ï¼ˆ{len(medium_heat)}æ¡ï¼‰")
        report.append(f"")
        for i, item in enumerate(medium_heat, 1):
            report.append(f"{i}. **{item['title']}** ({item['source']}, çƒ­åº¦{item['heat']})")
            report.append(f"   - æ¨èï¼š{', '.join(item['categories'])}")
            report.append(f"   - {item['url']}")
            report.append(f"")
    
    # ä½çƒ­åº¦ï¼ˆä»…åˆ—æ ‡é¢˜ï¼‰
    if low_heat:
        report.append(f"## ğŸ’¤ ä½çƒ­åº¦ï¼ˆ{len(low_heat)}æ¡ï¼‰")
        report.append(f"")
        for item in low_heat[:10]:
            report.append(f"- {item['title']} ({item['source']})")
        report.append(f"")
    
    # ç»Ÿè®¡
    report.append(f"---")
    report.append(f"")
    report.append(f"## ğŸ“Š ç»Ÿè®¡")
    report.append(f"")
    
    category_counter = Counter()
    for item in items_sorted:
        for cat in item['categories']:
            category_counter[cat] += 1
    
    report.append(f"**å†…å®¹çº¿åˆ†å¸ƒ**ï¼š")
    for cat, count in category_counter.most_common():
        report.append(f"- {cat}: {count}æ¡")
    report.append(f"")
    
    source_counter = Counter([x['source'] for x in items_sorted])
    report.append(f"**æ•°æ®æºåˆ†å¸ƒ**ï¼š")
    for source, count in source_counter.most_common():
        report.append(f"- {source}: {count}æ¡")
    
    return "\n".join(report)

# ============================================
# ä¸»ç¨‹åº
# ============================================

def run_monitor():
    """è¿è¡Œç›‘æ§"""
    print(f"\n{'='*60}")
    print(f"ğŸ¤– AIçƒ­ç‚¹ç›‘æ§ç³»ç»Ÿ")
    print(f"{'='*60}\n")
    
    all_items = []
    
    # é‡‡é›†æ•°æ®
    all_items.extend(fetch_hackernews())
    all_items.extend(fetch_brave_search())
    all_items.extend(fetch_producthunt())
    all_items.extend(fetch_arxiv())
    all_items.extend(fetch_techcrunch())
    
    # å»é‡
    all_items = deduplicate(all_items)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æ•°æ®é‡‡é›†å®Œæˆï¼šå…± {len(all_items)} æ¡ï¼ˆå»é‡åï¼‰")
    print(f"{'='*60}\n")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_report(all_items)
    
    # ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_file = f"ai_trending_{timestamp}.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆï¼š{report_file}\n")
    
    # ä¿å­˜JSON
    json_file = f"ai_trending_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(all_items, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ•°æ®å·²ä¿å­˜ï¼š{json_file}\n")
    
    return report_file, json_file

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--daemon":
        print("ğŸ”„ åå°è¿è¡Œæ¨¡å¼ï¼ˆæ¯4å°æ—¶æ‰§è¡Œä¸€æ¬¡ï¼‰")
        print("æŒ‰ Ctrl+C åœæ­¢\n")
        
        while True:
            try:
                run_monitor()
                print(f"\nâ° ä¸‹æ¬¡è¿è¡Œæ—¶é—´ï¼š{(datetime.now() + timedelta(hours=4)).strftime('%Y-%m-%d %H:%M:%S')}")
                time.sleep(4 * 3600)
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç›‘æ§å·²åœæ­¢")
                break
    else:
        run_monitor()
```

---

## ğŸ“– ä½¿ç”¨æŒ‡å—

### ç¬¬ä¸€æ­¥ï¼šé…ç½®API Key

**å¿…éœ€**ï¼š
- âœ… Brave Search APIï¼ˆå…è´¹ï¼Œæ¨èï¼‰
  - æ³¨å†Œï¼šhttps://brave.com/search/api/
  - å…è´¹é¢åº¦ï¼š2000æ¬¡/æœˆ

**å¯é€‰**ï¼ˆå¢å¼ºåŠŸèƒ½ï¼‰ï¼š
- âš ï¸ Reddit APIï¼ˆéœ€è¦åˆ›å»ºåº”ç”¨ï¼‰
- âš ï¸ Twitter API v2ï¼ˆä»˜è´¹ï¼Œ$100/æœˆèµ·ï¼‰

### ç¬¬äºŒæ­¥ï¼šå®‰è£…ä¾èµ–

```bash
pip install requests feedparser
```

### ç¬¬ä¸‰æ­¥ï¼šè¿è¡Œç›‘æ§

```bash
# è¿è¡Œä¸€æ¬¡
python ai_monitor.py

# åå°è¿è¡Œï¼ˆæ¯4å°æ—¶ï¼‰
python ai_monitor.py --daemon
```

### ç¬¬å››æ­¥ï¼šæŸ¥çœ‹æŠ¥å‘Š

ç”Ÿæˆçš„æ–‡ä»¶ï¼š
- `ai_trending_20260221_120000.md`ï¼ˆMarkdownæŠ¥å‘Šï¼‰
- `ai_trending_20260221_120000.json`ï¼ˆåŸå§‹æ•°æ®ï¼‰

### ç¬¬äº”æ­¥ï¼šå®šæ—¶ä»»åŠ¡ï¼ˆæ¨èï¼‰

**Linux/Macï¼ˆä½¿ç”¨cronï¼‰**ï¼š
```bash
# ç¼–è¾‘crontab
crontab -e

# æ·»åŠ å®šæ—¶ä»»åŠ¡ï¼ˆæ¯å¤©æ—©ä¸Š9ç‚¹è¿è¡Œï¼‰
0 9 * * * cd /path/to/workspace && python ai_monitor.py
```

**Windowsï¼ˆä½¿ç”¨ä»»åŠ¡è®¡åˆ’ç¨‹åºï¼‰**ï¼š
1. æ‰“å¼€"ä»»åŠ¡è®¡åˆ’ç¨‹åº"
2. åˆ›å»ºåŸºæœ¬ä»»åŠ¡
3. è§¦å‘å™¨ï¼šæ¯å¤©æ—©ä¸Š9ç‚¹
4. æ“ä½œï¼šå¯åŠ¨ç¨‹åº `python.exe ai_monitor.py`

---

## ğŸ“Š è¾“å‡ºç¤ºä¾‹

### MarkdownæŠ¥å‘Šç¤ºä¾‹

```markdown
# AIçƒ­ç‚¹æ—¥æŠ¥

> ç”Ÿæˆæ—¶é—´ï¼š2026-02-21 12:00:00
> æ•°æ®æºï¼šHackerNews, ProductHunt, arXiv, TechCrunch, Brave Search
> æ€»è®¡ï¼š25 æ¡çƒ­ç‚¹

---

## ğŸ”¥ğŸ”¥ğŸ”¥ é«˜çƒ­åº¦ï¼ˆ3æ¡ï¼‰

### 1. Google Unveils Project Genie: AI World Model

- **æ¥æº**ï¼šHackerNews
- **çƒ­åº¦**ï¼š8750
- **æ¨èå†…å®¹çº¿**ï¼šAIç§‘æ™®, AIå‡ºæµ·åˆ›ä¸š
- **é“¾æ¥**ï¼šhttps://...
- **ç®€ä»‹**ï¼šGoogleå‘å¸ƒä¸–ç•Œæ¨¡å‹Project Genie...

### 2. Anthropic Claude Now Supports 10M Token Context

- **æ¥æº**ï¼šTechCrunch
- **çƒ­åº¦**ï¼š6200
- **æ¨èå†…å®¹çº¿**ï¼šAIå·¥å…·, AIç¼–ç¨‹
- **é“¾æ¥**ï¼šhttps://...

...
```

---

## ğŸ”§ é«˜çº§é…ç½®

### è‡ªå®šä¹‰å…³é”®è¯

ç¼–è¾‘`ai_monitor.py`ï¼š
```python
AI_KEYWORDS = [
    "ä½ çš„è‡ªå®šä¹‰å…³é”®è¯1",
    "ä½ çš„è‡ªå®šä¹‰å…³é”®è¯2",
    # ...
]
```

### è°ƒæ•´çƒ­åº¦é˜ˆå€¼

```python
TRENDING_THRESHOLD = {
    "high": 10000,   # è°ƒé«˜é˜ˆå€¼
    "medium": 2000,
    "low": 500
}
```

### å¢åŠ æ–°æ•°æ®æº

```python
def fetch_your_source():
    """ä½ çš„è‡ªå®šä¹‰æ•°æ®æº"""
    # å®ç°æŠ“å–é€»è¾‘
    return []

# åœ¨run_monitor()ä¸­è°ƒç”¨
all_items.extend(fetch_your_source())
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥ä¼˜åŒ–

- [ ] å¢åŠ æœºå™¨å­¦ä¹ é¢„æµ‹ï¼ˆåŸºäºå†å²æ•°æ®é¢„æµ‹çˆ†æ¬¾æ¦‚ç‡ï¼‰
- [ ] ä¼ä¸šå¾®ä¿¡/é‚®ä»¶æ¨é€
- [ ] å¯è§†åŒ–Dashboardï¼ˆFlask + Chart.jsï¼‰
- [ ] å¤šè¯­è¨€æ”¯æŒï¼ˆä¸­æ–‡çƒ­ç‚¹è¿½è¸ªï¼‰
- [ ] ç«å“è¿½è¸ªï¼ˆç‰¹å®šå…¬å¸/äº§å“åŠ¨æ€ï¼‰

---

**æ–‡æ¡£çŠ¶æ€**ï¼šâœ…å·²å®Œæˆ  
**å…³è”æ–‡æ¡£**ï¼š
- `ai_monitor.py`ï¼ˆç›‘æ§è„šæœ¬ï¼‰
- `topic_scorer.py`ï¼ˆè¯„åˆ†è„šæœ¬ï¼‰
- `topic_selection_system.md`ï¼ˆç­›é€‰æ ‡å‡†ï¼‰
