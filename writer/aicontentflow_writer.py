#!/usr/bin/env python3
"""
å¤šæºä¿¡æ¯éªŒè¯æ£€ç´¢ç³»ç»Ÿ
åŠŸèƒ½ï¼šä¸‰å±‚éªŒè¯ä½“ç³» + è‡ªåŠ¨äº¤å‰éªŒè¯ + ç»“æ„åŒ–è¾“å‡º
ä½œè€…ï¼šAI Content Workflow
ç‰ˆæœ¬ï¼š2.0
"""

import os
import json
import subprocess
from typing import List, Dict, Optional
from datetime import datetime

# ============================================================================
# é…ç½®åŒº
# ============================================================================

BRAVE_API_KEY = os.getenv("BRAVE_API_KEY", "")

# ä¿¡æ¯æºé…ç½®
SOURCES_CONFIG = {
    "tier1_official": {
        "google_ai": ["site:ai.googleblog.com", "site:deepmind.com", "site:research.google"],
        "openai": ["site:openai.com/blog", "site:platform.openai.com"],
        "meta_ai": ["site:ai.meta.com", "site:research.facebook.com"],
        "microsoft_ai": ["site:blogs.microsoft.com/ai"],
        "anthropic": ["site:anthropic.com"],
    },
    "tier1_academic": {
        "arxiv": ["site:arxiv.org"],
        "nature": ["site:nature.com"],
        "papers_with_code": ["site:paperswithcode.com"],
    },
    "tier2_media": {
        "tech_media": ["site:techcrunch.com", "site:theverge.com", "site:wired.com"],
        "deep_analysis": ["site:technologyreview.com", "site:spectrum.ieee.org"],
        "chinese_media": ["æœºå™¨ä¹‹å¿ƒ", "é‡å­ä½", "æ–°æ™ºå…ƒ"],
    },
    "tier3_community": {
        "hacker_news": ["site:news.ycombinator.com"],
        "reddit": ["site:reddit.com/r/MachineLearning", "site:reddit.com/r/artificial"],
        "github": ["site:github.com"],
    }
}

# æ—¶æ•ˆæ€§è¿‡æ»¤é…ç½®
FRESHNESS_OPTIONS = {
    "past_day": "pd",
    "past_week": "pw",
    "past_month": "pm",
    "past_year": "py"
}

# ============================================================================
# æ ¸å¿ƒå‡½æ•°
# ============================================================================

def brave_search(query: str, sites: List[str] = None, freshness: str = "pm", count: int = 10) -> Dict:
    """
    è°ƒç”¨ Brave Search API
    
    å‚æ•°:
        query: æœç´¢å…³é”®è¯
        sites: ç«™ç‚¹åˆ—è¡¨ï¼ˆå¦‚ ["site:arxiv.org"]ï¼‰
        freshness: æ—¶æ•ˆæ€§ï¼ˆpd/pw/pm/pyï¼‰
        count: è¿”å›ç»“æœæ•°é‡
    
    è¿”å›:
        {
            "success": bool,
            "results": List[Dict],
            "error": str
        }
    """
    try:
        from urllib.parse import quote
        
        # æ„å»ºæœç´¢æŸ¥è¯¢
        if sites:
            site_filter = " OR ".join(sites)
            full_query = f"({query}) AND ({site_filter})"
        else:
            full_query = query
        
        # URLç¼–ç 
        encoded_query = quote(full_query)
        
        # è°ƒç”¨ Brave Search API
        cmd = [
            "curl", "-s",
            "-H", f"X-Subscription-Token: {BRAVE_API_KEY}",
            f"https://api.search.brave.com/res/v1/web/search?q={encoded_query}&freshness={freshness}&count={count}"
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        
        if result.returncode != 0:
            return {"success": False, "results": [], "error": result.stderr}
        
        data = json.loads(result.stdout)
        
        # æå–ç»“æœ
        results = []
        for item in data.get("web", {}).get("results", []):
            results.append({
                "title": item.get("title"),
                "url": item.get("url"),
                "description": item.get("description"),
                "published": item.get("age", "æœªçŸ¥"),
            })
        
        return {"success": True, "results": results, "error": None}
    
    except Exception as e:
        return {"success": False, "results": [], "error": str(e)}


def web_search_fallback(query: str) -> Dict:
    """
    å¤‡ç”¨ï¼šä½¿ç”¨é€šç”¨ web_searchï¼ˆå½“ Brave API å¤±è´¥æ—¶ï¼‰
    æ³¨æ„ï¼šè¿™éœ€è¦åœ¨ AI ç¯å¢ƒä¸­è°ƒç”¨ï¼Œè¿™é‡Œä»…ä½œå ä½
    """
    return {
        "success": False,
        "results": [],
        "error": "éœ€è¦åœ¨ AI ç¯å¢ƒä¸­è°ƒç”¨ web_search å·¥å…·"
    }


def multi_source_research(
    topic: str,
    keyword: str,
    freshness: str = "pm",
    enable_official: bool = True,
    enable_academic: bool = True,
    enable_media: bool = True,
    enable_community: bool = True,
) -> Dict:
    """
    å¤šæºä¿¡æ¯æ£€ç´¢ä¸»å‡½æ•°
    
    å‚æ•°:
        topic: è¯é¢˜æè¿°ï¼ˆç”¨äºè¯­ä¹‰æ£€ç´¢ï¼‰
        keyword: å…³é”®è¯ï¼ˆç”¨äºå…³é”®è¯æ£€ç´¢ï¼‰
        freshness: æ—¶æ•ˆæ€§ï¼ˆpd=1å¤©, pw=1å‘¨, pm=1æœˆ, py=1å¹´ï¼‰
        enable_*: æ˜¯å¦å¯ç”¨å„å±‚çº§ä¿¡æ¯æº
    
    è¿”å›:
        {
            "topic": str,
            "timestamp": str,
            "tier1_official": {...},
            "tier1_academic": {...},
            "tier2_media": {...},
            "tier3_community": {...},
            "summary": {...}
        }
    """
    report = {
        "topic": topic,
        "keyword": keyword,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "tier1_official": {},
        "tier1_academic": {},
        "tier2_media": {},
        "tier3_community": {},
        "summary": {
            "total_sources": 0,
            "successful_sources": 0,
            "total_results": 0,
        }
    }
    
    # ç¬¬ä¸€å±‚ï¼šå®˜æ–¹æº
    if enable_official:
        print("ğŸ” æ­£åœ¨æ£€ç´¢ç¬¬ä¸€å±‚ï¼šå®˜æ–¹æƒå¨æº...")
        for source_name, sites in SOURCES_CONFIG["tier1_official"].items():
            print(f"  â””â”€ æ£€ç´¢ {source_name}...")
            result = brave_search(keyword, sites, freshness, count=5)
            report["tier1_official"][source_name] = result
            
            if result["success"]:
                report["summary"]["successful_sources"] += 1
                report["summary"]["total_results"] += len(result["results"])
            report["summary"]["total_sources"] += 1
    
    # ç¬¬ä¸€å±‚ï¼šå­¦æœ¯æº
    if enable_academic:
        print("ğŸ” æ­£åœ¨æ£€ç´¢ç¬¬ä¸€å±‚ï¼šå­¦æœ¯æƒå¨æº...")
        for source_name, sites in SOURCES_CONFIG["tier1_academic"].items():
            print(f"  â””â”€ æ£€ç´¢ {source_name}...")
            result = brave_search(keyword, sites, freshness, count=5)
            report["tier1_academic"][source_name] = result
            
            if result["success"]:
                report["summary"]["successful_sources"] += 1
                report["summary"]["total_results"] += len(result["results"])
            report["summary"]["total_sources"] += 1
    
    # ç¬¬äºŒå±‚ï¼šåª’ä½“æº
    if enable_media:
        print("ğŸ” æ­£åœ¨æ£€ç´¢ç¬¬äºŒå±‚ï¼šä¸“ä¸šåª’ä½“æº...")
        for source_name, sites in SOURCES_CONFIG["tier2_media"].items():
            print(f"  â””â”€ æ£€ç´¢ {source_name}...")
            # ä¸­æ–‡åª’ä½“ä¸ä½¿ç”¨ç«™ç‚¹è¿‡æ»¤
            if source_name == "chinese_media":
                result = brave_search(f"{keyword} {' OR '.join(sites)}", None, freshness, count=5)
            else:
                result = brave_search(keyword, sites, freshness, count=5)
            report["tier2_media"][source_name] = result
            
            if result["success"]:
                report["summary"]["successful_sources"] += 1
                report["summary"]["total_results"] += len(result["results"])
            report["summary"]["total_sources"] += 1
    
    # ç¬¬ä¸‰å±‚ï¼šç¤¾åŒºæº
    if enable_community:
        print("ğŸ” æ­£åœ¨æ£€ç´¢ç¬¬ä¸‰å±‚ï¼šç¤¾åŒºéªŒè¯æº...")
        for source_name, sites in SOURCES_CONFIG["tier3_community"].items():
            print(f"  â””â”€ æ£€ç´¢ {source_name}...")
            result = brave_search(keyword, sites, freshness, count=5)
            report["tier3_community"][source_name] = result
            
            if result["success"]:
                report["summary"]["successful_sources"] += 1
                report["summary"]["total_results"] += len(result["results"])
            report["summary"]["total_sources"] += 1
    
    return report


def generate_markdown_report(report: Dict, output_file: str = None) -> str:
    """
    ç”Ÿæˆ Markdown æ ¼å¼çš„è°ƒç ”æŠ¥å‘Š
    """
    md = []
    
    # æ ‡é¢˜
    md.append(f"# å¤šæºä¿¡æ¯æ£€ç´¢æŠ¥å‘Š")
    md.append(f"\n**è¯é¢˜**: {report['topic']}")
    md.append(f"**å…³é”®è¯**: {report['keyword']}")
    md.append(f"**ç”Ÿæˆæ—¶é—´**: {report['timestamp']}")
    md.append(f"\n---\n")
    
    # æ‘˜è¦
    md.append(f"## ğŸ“Š æ£€ç´¢æ‘˜è¦")
    md.append(f"- **æ£€ç´¢ä¿¡æ¯æºæ€»æ•°**: {report['summary']['total_sources']}")
    md.append(f"- **æˆåŠŸæ£€ç´¢æºæ•°**: {report['summary']['successful_sources']}")
    md.append(f"- **è·å–ç»“æœæ€»æ•°**: {report['summary']['total_results']}")
    md.append(f"\n---\n")
    
    # ç¬¬ä¸€å±‚ï¼šå®˜æ–¹æº
    md.append(f"## ğŸ›ï¸ ç¬¬ä¸€å±‚ï¼šå®˜æ–¹æƒå¨æº")
    for source_name, result in report["tier1_official"].items():
        md.append(f"\n### {source_name.replace('_', ' ').title()}")
        if result["success"] and result["results"]:
            for idx, item in enumerate(result["results"], 1):
                md.append(f"{idx}. **{item['title']}**")
                md.append(f"   - é“¾æ¥: {item['url']}")
                md.append(f"   - å‘å¸ƒ: {item['published']}")
                md.append(f"   - æ‘˜è¦: {item['description'][:150]}...")
        else:
            md.append(f"âŒ æœªè·å–åˆ°ç»“æœï¼ˆåŸå› : {result['error']}ï¼‰")
    
    # ç¬¬ä¸€å±‚ï¼šå­¦æœ¯æº
    md.append(f"\n## ğŸ“š ç¬¬ä¸€å±‚ï¼šå­¦æœ¯æƒå¨æº")
    for source_name, result in report["tier1_academic"].items():
        md.append(f"\n### {source_name.replace('_', ' ').title()}")
        if result["success"] and result["results"]:
            for idx, item in enumerate(result["results"], 1):
                md.append(f"{idx}. **{item['title']}**")
                md.append(f"   - é“¾æ¥: {item['url']}")
                md.append(f"   - å‘å¸ƒ: {item['published']}")
                md.append(f"   - æ‘˜è¦: {item['description'][:150]}...")
        else:
            md.append(f"âŒ æœªè·å–åˆ°ç»“æœï¼ˆåŸå› : {result['error']}ï¼‰")
    
    # ç¬¬äºŒå±‚ï¼šåª’ä½“æº
    md.append(f"\n## ğŸ“° ç¬¬äºŒå±‚ï¼šä¸“ä¸šåª’ä½“æº")
    for source_name, result in report["tier2_media"].items():
        md.append(f"\n### {source_name.replace('_', ' ').title()}")
        if result["success"] and result["results"]:
            for idx, item in enumerate(result["results"], 1):
                md.append(f"{idx}. **{item['title']}**")
                md.append(f"   - é“¾æ¥: {item['url']}")
                md.append(f"   - å‘å¸ƒ: {item['published']}")
                md.append(f"   - æ‘˜è¦: {item['description'][:150]}...")
        else:
            md.append(f"âŒ æœªè·å–åˆ°ç»“æœï¼ˆåŸå› : {result['error']}ï¼‰")
    
    # ç¬¬ä¸‰å±‚ï¼šç¤¾åŒºæº
    md.append(f"\n## ğŸ’¬ ç¬¬ä¸‰å±‚ï¼šç¤¾åŒºéªŒè¯æº")
    for source_name, result in report["tier3_community"].items():
        md.append(f"\n### {source_name.replace('_', ' ').title()}")
        if result["success"] and result["results"]:
            for idx, item in enumerate(result["results"], 1):
                md.append(f"{idx}. **{item['title']}**")
                md.append(f"   - é“¾æ¥: {item['url']}")
                md.append(f"   - å‘å¸ƒ: {item['published']}")
                md.append(f"   - æ‘˜è¦: {item['description'][:150]}...")
        else:
            md.append(f"âŒ æœªè·å–åˆ°ç»“æœï¼ˆåŸå› : {result['error']}ï¼‰")
    
    md_content = "\n".join(md)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    if output_file:
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(md_content)
        print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    
    return md_content


# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="å¤šæºä¿¡æ¯éªŒè¯æ£€ç´¢ç³»ç»Ÿ")
    parser.add_argument("topic", help="è¯é¢˜æè¿°")
    parser.add_argument("keyword", help="æ£€ç´¢å…³é”®è¯")
    parser.add_argument("--freshness", default="pm", choices=["pd", "pw", "pm", "py"], 
                        help="æ—¶æ•ˆæ€§: pd=1å¤©, pw=1å‘¨, pm=1æœˆ, py=1å¹´")
    parser.add_argument("--no-official", action="store_true", help="ç¦ç”¨å®˜æ–¹æº")
    parser.add_argument("--no-academic", action="store_true", help="ç¦ç”¨å­¦æœ¯æº")
    parser.add_argument("--no-media", action="store_true", help="ç¦ç”¨åª’ä½“æº")
    parser.add_argument("--no-community", action="store_true", help="ç¦ç”¨ç¤¾åŒºæº")
    parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆMarkdownæ ¼å¼ï¼‰")
    parser.add_argument("--json", action="store_true", help="è¾“å‡ºJSONæ ¼å¼")
    
    args = parser.parse_args()
    
    # æ‰§è¡Œæ£€ç´¢
    print(f"\nğŸš€ å¼€å§‹å¤šæºä¿¡æ¯æ£€ç´¢...")
    print(f"è¯é¢˜: {args.topic}")
    print(f"å…³é”®è¯: {args.keyword}")
    print(f"æ—¶æ•ˆæ€§: {args.freshness}\n")
    
    report = multi_source_research(
        topic=args.topic,
        keyword=args.keyword,
        freshness=args.freshness,
        enable_official=not args.no_official,
        enable_academic=not args.no_academic,
        enable_media=not args.no_media,
        enable_community=not args.no_community,
    )
    
    # è¾“å‡ºç»“æœ
    if args.json:
        print(json.dumps(report, ensure_ascii=False, indent=2))
    else:
        output_file = args.output or f"research_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        md_report = generate_markdown_report(report, output_file)
        print(f"\n" + "="*80)
        print(md_report)
