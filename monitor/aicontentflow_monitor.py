#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIçƒ­ç‚¹ç›‘æ§ç³»ç»Ÿ
æ¯æ—¥è‡ªåŠ¨æŠ“å–AIè¡Œä¸šåŠ¨æ€ï¼Œç”Ÿæˆå€™é€‰ä¸»é¢˜æŠ¥å‘Š

è¿è¡Œæ–¹å¼ï¼š
    python ai_monitor.py              # è¿è¡Œä¸€æ¬¡
    python ai_monitor.py --daemon     # åå°è¿è¡Œï¼ˆæ¯4å°æ—¶ï¼‰
    python ai_monitor.py --test       # æµ‹è¯•æ¨¡å¼ï¼ˆä»…HackerNewsï¼‰
"""

import requests
import feedparser
import json
import os
from datetime import datetime, timedelta
from collections import Counter
from typing import List, Dict
import time
import sys

# ============================================
# é…ç½®
# ============================================

# Brave Search APIé…ç½®ï¼ˆå¯é€‰ï¼‰
BRAVE_API_KEY = os.getenv("BRAVE_API_KEY", "")  # ä» https://brave.com/search/api/ è·å–ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡

# å…³é”®è¯é…ç½®
AI_KEYWORDS = [
    "AI", "artificial intelligence", "machine learning", "deep learning",
    "GPT", "Gemini", "Claude", "LLM", "large language model",
    "generative AI", "AI tools", "AI coding", "AI startup",
    "OpenAI", "Anthropic", "Google AI", "neural network"
]

# çƒ­åº¦é˜ˆå€¼
TRENDING_THRESHOLD = {
    "high": 5000,    # 24å°æ—¶å†…è®¨è®ºé‡ > 5000
    "medium": 1000,  # 1000-5000
    "low": 100       # 100-1000
}

# ============================================
# æ•°æ®é‡‡é›†æ¨¡å—
# ============================================

def fetch_hackernews():
    """æŠ“å–HackerNewsçƒ­é—¨è¯é¢˜"""
    print("ğŸ“° æ­£åœ¨æŠ“å– HackerNews...")
    
    try:
        url = "https://hacker-news.firebaseio.com/v0/topstories.json"
        story_ids = requests.get(url, timeout=10).json()[:30]
        
        stories = []
        for story_id in story_ids:
            story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            story = requests.get(story_url, timeout=10).json()
            
            if story and 'title' in story:
                title = story['title'].lower()
                if any(kw.lower() in title for kw in AI_KEYWORDS):
                    stories.append({
                        "source": "HackerNews",
                        "title": story['title'],
                        "url": story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                        "score": story.get('score', 0),
                        "comments": story.get('descendants', 0),
                        "timestamp": datetime.now().isoformat()
                    })
        
        print(f"  âœ… æ‰¾åˆ° {len(stories)} æ¡AIç›¸å…³è¯é¢˜")
        return stories
    
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥ï¼š{e}")
        return []

def fetch_brave_search():
    """ä½¿ç”¨Brave Search APIæœç´¢AIçƒ­ç‚¹"""
    print("ğŸ” æ­£åœ¨ä½¿ç”¨ Brave Search...")
    
    if not BRAVE_API_KEY or BRAVE_API_KEY == "":
        print("  âš ï¸  æœªé…ç½®Brave API Keyï¼Œè·³è¿‡")
        return []
    
    try:
        headers = {
            "Accept": "application/json",
            "X-Subscription-Token": BRAVE_API_KEY
        }
        
        results = []
        queries = ["AI news today", "new AI tools 2026", "AI startup"]
        
        for query in queries:
            url = f"https://api.search.brave.com/res/v1/web/search?q={query}&count=10"
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                for item in data.get('web', {}).get('results', []):
                    results.append({
                        "source": "Brave Search",
                        "title": item.get('title'),
                        "url": item.get('url'),
                        "description": item.get('description'),
                        "timestamp": datetime.now().isoformat()
                    })
        
        print(f"  âœ… æ‰¾åˆ° {len(results)} æ¡æœç´¢ç»“æœ")
        return results
    
    except Exception as e:
        print(f"  âŒ æœç´¢å¤±è´¥ï¼š{e}")
        return []

def fetch_producthunt():
    """æŠ“å–ProductHunt AIå·¥å…·"""
    print("ğŸš€ æ­£åœ¨æŠ“å– ProductHunt...")
    
    try:
        feed = feedparser.parse("https://www.producthunt.com/topics/artificial-intelligence.rss")
        
        tools = []
        for entry in feed.entries[:10]:
            tools.append({
                "source": "ProductHunt",
                "title": entry.title,
                "url": entry.link,
                "description": entry.get('summary', ''),
                "timestamp": entry.get('published', datetime.now().isoformat())
            })
        
        print(f"  âœ… æ‰¾åˆ° {len(tools)} ä¸ªAIå·¥å…·")
        return tools
    
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥ï¼š{e}")
        return []

def fetch_arxiv():
    """æŠ“å–arXiv AIè®ºæ–‡"""
    print("ğŸ“š æ­£åœ¨æŠ“å– arXiv...")
    
    try:
        feeds = [
            "http://export.arxiv.org/rss/cs.AI",
            "http://export.arxiv.org/rss/cs.CL",
            "http://export.arxiv.org/rss/cs.LG"
        ]
        
        papers = []
        for feed_url in feeds:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries[:5]:
                papers.append({
                    "source": "arXiv",
                    "title": entry.title,
                    "url": entry.link,
                    "description": entry.get('summary', ''),
                    "timestamp": entry.get('published', datetime.now().isoformat())
                })
        
        print(f"  âœ… æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥ï¼š{e}")
        return []

def fetch_techcrunch():
    """æŠ“å–TechCrunch AIæ–°é—»"""
    print("ğŸ“° æ­£åœ¨æŠ“å– TechCrunch...")
    
    try:
        feed = feedparser.parse("https://techcrunch.com/category/artificial-intelligence/feed/")
        
        news = []
        for entry in feed.entries[:10]:
            news.append({
                "source": "TechCrunch",
                "title": entry.title,
                "url": entry.link,
                "description": entry.get('summary', ''),
                "timestamp": entry.get('published', datetime.now().isoformat())
            })
        
        print(f"  âœ… æ‰¾åˆ° {len(news)} æ¡æ–°é—»")
        return news
    
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥ï¼š{e}")
        return []

def fetch_github_trending():
    """æŠ“å–GitHub Trending AIé¡¹ç›®"""
    print("ğŸŒŸ æ­£åœ¨æŠ“å– GitHub Trending...")
    
    try:
        from bs4 import BeautifulSoup
        
        # æŠ“å–Pythonå’ŒJavaScriptçš„AIé¡¹ç›®
        languages = ['python', 'javascript', 'typescript']
        projects = []
        
        for lang in languages:
            url = f"https://github.com/trending/{lang}?since=daily"
            
            try:
                response = requests.get(url, timeout=15, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })
                
                if response.status_code != 200:
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾æ‰€æœ‰ä»“åº“æ¡ç›®
                repos = soup.select('article.Box-row')
                
                for repo in repos[:10]:  # æ¯ç§è¯­è¨€å–å‰10ä¸ª
                    try:
                        # æå–æ ‡é¢˜å’Œé“¾æ¥
                        title_elem = repo.select_one('h2 a')
                        if not title_elem:
                            continue
                        
                        repo_name = title_elem.get_text(strip=True).replace('\n', '').replace(' ', '')
                        repo_url = "https://github.com" + title_elem.get('href', '')
                        
                        # æå–æè¿°
                        desc_elem = repo.select_one('p')
                        description = desc_elem.get_text(strip=True) if desc_elem else ''
                        
                        # æå–Starsï¼ˆä»Šæ—¥æ–°å¢ï¼‰
                        stars_elem = repo.select_one('span.d-inline-block.float-sm-right')
                        stars_today = stars_elem.get_text(strip=True) if stars_elem else '0'
                        
                        # åªä¿ç•™AIç›¸å…³é¡¹ç›®
                        full_text = (repo_name + " " + description).lower()
                        if any(kw.lower() in full_text for kw in AI_KEYWORDS):
                            projects.append({
                                "source": "GitHub Trending",
                                "title": repo_name,
                                "url": repo_url,
                                "description": description,
                                "language": lang,
                                "stars_today": stars_today,
                                "timestamp": datetime.now().isoformat()
                            })
                    
                    except Exception as e:
                        continue
                
            except Exception as e:
                print(f"  âš ï¸  æŠ“å– {lang} å¤±è´¥ï¼š{e}")
                continue
        
        # å»é‡ï¼ˆå¯èƒ½åŒä¸€ä¸ªé¡¹ç›®åœ¨å¤šä¸ªè¯­è¨€åˆ†ç±»ä¸­ï¼‰
        seen_urls = set()
        unique_projects = []
        for proj in projects:
            if proj['url'] not in seen_urls:
                seen_urls.add(proj['url'])
                unique_projects.append(proj)
        
        print(f"  âœ… æ‰¾åˆ° {len(unique_projects)} ä¸ªAIé¡¹ç›®")
        return unique_projects
    
    except ImportError:
        print(f"  âŒ ç¼ºå°‘ä¾èµ–ï¼špip install beautifulsoup4")
        return []
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥ï¼š{e}")
        return []

# ============================================
# æ•°æ®å¤„ç†æ¨¡å—
# ============================================

def calculate_heat(item: Dict) -> int:
    """è®¡ç®—çƒ­åº¦åˆ†æ•°"""
    heat = 0
    
    # HackerNewsçƒ­åº¦
    if item['source'] == 'HackerNews':
        heat = item.get('score', 0) + item.get('comments', 0) * 2
    
    # å…¶ä»–æ¥æºåŸºç¡€åˆ†
    else:
        heat = 100
    
    # æ—¶æ•ˆæ€§åŠ æˆ
    try:
        timestamp = datetime.fromisoformat(item['timestamp'].replace('Z', '+00:00'))
        hours_ago = (datetime.now() - timestamp.replace(tzinfo=None)).total_seconds() / 3600
        if hours_ago < 24:
            heat *= 2
        elif hours_ago < 72:
            heat *= 1.5
    except:
        pass
    
    return int(heat)

def classify_topic(title: str, description: str = "") -> List[str]:
    """åˆ†ç±»åˆ°å†…å®¹çº¿"""
    text = (title + " " + description).lower()
    
    categories = []
    
    # AIç§‘æ™®å…³é”®è¯
    if any(kw in text for kw in ['explain', 'understand', 'what is', 'how does', 'introduction', 'guide']):
        categories.append("AIç§‘æ™®")
    
    # AIå·¥å…·å…³é”®è¯
    if any(kw in text for kw in ['tool', 'app', 'software', 'platform', 'api', 'product', 'launch']):
        categories.append("AIå·¥å…·")
    
    # AIç¼–ç¨‹å…³é”®è¯
    if any(kw in text for kw in ['code', 'coding', 'programming', 'developer', 'github', 'open source', 'library']):
        categories.append("AIç¼–ç¨‹")
    
    # AIå‡ºæµ·åˆ›ä¸šå…³é”®è¯
    if any(kw in text for kw in ['startup', 'funding', 'market', 'business', 'revenue', 'valuation', 'raises']):
        categories.append("AIå‡ºæµ·åˆ›ä¸š")
    
    # é»˜è®¤åˆ†ç±»
    if not categories:
        categories.append("AIç§‘æ™®")
    
    return categories

def deduplicate(items: List[Dict]) -> List[Dict]:
    """å»é‡"""
    seen_titles = set()
    unique_items = []
    
    for item in items:
        # ç®€å•å»é‡ï¼šæ¯”è¾ƒæ ‡é¢˜å‰50ä¸ªå­—ç¬¦
        title_key = item['title'][:50].lower()
        if title_key not in seen_titles:
            seen_titles.add(title_key)
            unique_items.append(item)
    
    return unique_items

# ============================================
# æŠ¥å‘Šç”Ÿæˆæ¨¡å—
# ============================================

def generate_report(items: List[Dict]) -> str:
    """ç”ŸæˆMarkdownæŠ¥å‘Š"""
    
    # æŒ‰çƒ­åº¦æ’åº
    items_with_heat = []
    for item in items:
        item['heat'] = calculate_heat(item)
        item['categories'] = classify_topic(item['title'], item.get('description', ''))
        items_with_heat.append(item)
    
    items_sorted = sorted(items_with_heat, key=lambda x: x['heat'], reverse=True)
    
    # åˆ†çº§
    high_heat = [x for x in items_sorted if x['heat'] >= TRENDING_THRESHOLD['high']]
    medium_heat = [x for x in items_sorted if TRENDING_THRESHOLD['medium'] <= x['heat'] < TRENDING_THRESHOLD['high']]
    low_heat = [x for x in items_sorted if TRENDING_THRESHOLD['low'] <= x['heat'] < TRENDING_THRESHOLD['medium']]
    
    # ============ æ–°å¢ï¼šTop 5 æ¨èé€‰é¢˜ ============
    # ä¼˜å…ˆä»é«˜çƒ­åº¦å’Œä¸­çƒ­åº¦ä¸­é€‰æ‹©ï¼Œç¡®ä¿æœ‰5ä¸ªé€‰é¢˜
    top_candidates = high_heat + medium_heat + low_heat
    top_5 = top_candidates[:5] if len(top_candidates) >= 5 else top_candidates
    
    # ç”ŸæˆæŠ¥å‘Š
    report = []
    report.append(f"# AIçƒ­ç‚¹æ—¥æŠ¥")
    report.append(f"")
    report.append(f"> ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"> æ•°æ®æºï¼šHackerNews, ProductHunt, arXiv, TechCrunch, GitHub Trending, Brave Search")
    report.append(f"> æ€»è®¡ï¼š{len(items_sorted)} æ¡çƒ­ç‚¹")
    report.append(f"")
    report.append(f"---")
    report.append(f"")
    
    # ============ æ¨èé€‰é¢˜ Top 5 ============
    report.append(f"## ğŸ¯ ä»Šæ—¥æ¨èé€‰é¢˜ï¼ˆTop 5ï¼‰")
    report.append(f"")
    
    if len(top_5) > 0:
        for i, item in enumerate(top_5, 1):
            # çƒ­åº¦æ˜Ÿçº§æ˜¾ç¤º
            if item['heat'] >= TRENDING_THRESHOLD['high']:
                heat_stars = "â­â­â­"
            elif item['heat'] >= TRENDING_THRESHOLD['medium']:
                heat_stars = "â­â­"
            else:
                heat_stars = "â­"
            
            report.append(f"### {i}. {item['title']}")
            report.append(f"")
            report.append(f"- **çƒ­åº¦**: {heat_stars} ({item['heat']})")
            report.append(f"- **æ¥æº**: {item['source']}")
            report.append(f"- **æ¨èæ–¹å‘**: {', '.join(item['categories'])}")
            report.append(f"- **é“¾æ¥**: {item['url']}")
            
            # æ˜¾ç¤ºç®€ä»‹ï¼ˆå¦‚æœæœ‰ï¼‰
            if item.get('description'):
                desc = item['description'][:150].strip()
                if desc:
                    report.append(f"- **ç®€ä»‹**: {desc}...")
            
            report.append(f"")
    else:
        report.append(f"*ä»Šæ—¥æš‚æ— æ¨èé€‰é¢˜*")
        report.append(f"")
    
    report.append(f"---")
    report.append(f"")
    report.append(f"## â“ è¯·é€‰æ‹©åˆ›ä½œæ–¹å‘")
    report.append(f"")
    report.append(f"è¯·å›å¤ **æ•°å­—1-5** ç¡®è®¤é€‰é¢˜ï¼Œæˆ–å›å¤ **ã€Œç­‰æ˜å¤©ã€** è·³è¿‡ä»Šæ—¥åˆ›ä½œã€‚")
    report.append(f"")
    report.append(f"---")
    report.append(f"")
    
    # é«˜çƒ­åº¦
    if high_heat:
        report.append(f"## ğŸ”¥ğŸ”¥ğŸ”¥ é«˜çƒ­åº¦ï¼ˆ{len(high_heat)}æ¡ï¼‰")
        report.append(f"")
        for i, item in enumerate(high_heat, 1):
            report.append(f"### {i}. {item['title']}")
            report.append(f"")
            report.append(f"- **æ¥æº**ï¼š{item['source']}")
            report.append(f"- **çƒ­åº¦**ï¼š{item['heat']}")
            report.append(f"- **æ¨èå†…å®¹çº¿**ï¼š{', '.join(item['categories'])}")
            report.append(f"- **é“¾æ¥**ï¼š{item['url']}")
            if item.get('description'):
                desc = item['description'][:200].strip()
                if desc:
                    report.append(f"- **ç®€ä»‹**ï¼š{desc}...")
            report.append(f"")
    else:
        report.append(f"## ğŸ”¥ğŸ”¥ğŸ”¥ é«˜çƒ­åº¦ï¼ˆ0æ¡ï¼‰")
        report.append(f"")
        report.append(f"*ä»Šæ—¥æš‚æ— é«˜çƒ­åº¦è¯é¢˜*")
        report.append(f"")
    
    # ä¸­çƒ­åº¦
    if medium_heat:
        report.append(f"## ğŸ”¥ ä¸­çƒ­åº¦ï¼ˆ{len(medium_heat)}æ¡ï¼‰")
        report.append(f"")
        for i, item in enumerate(medium_heat, 1):
            report.append(f"{i}. **{item['title']}** ({item['source']}, çƒ­åº¦{item['heat']})")
            report.append(f"   - æ¨èï¼š{', '.join(item['categories'])}")
            report.append(f"   - {item['url']}")
            report.append(f"")
    
    # ä½çƒ­åº¦ï¼ˆä»…åˆ—æ ‡é¢˜ï¼‰
    if low_heat:
        report.append(f"## ğŸ’¤ ä½çƒ­åº¦ï¼ˆ{len(low_heat)}æ¡ï¼‰")
        report.append(f"")
        for item in low_heat[:10]:
            report.append(f"- {item['title']} ({item['source']})")
        if len(low_heat) > 10:
            report.append(f"- ... è¿˜æœ‰ {len(low_heat)-10} æ¡")
        report.append(f"")
    
    # ç»Ÿè®¡
    report.append(f"---")
    report.append(f"")
    report.append(f"## ğŸ“Š ç»Ÿè®¡")
    report.append(f"")
    
    category_counter = Counter()
    for item in items_sorted:
        for cat in item['categories']:
            category_counter[cat] += 1
    
    report.append(f"**å†…å®¹çº¿åˆ†å¸ƒ**ï¼š")
    for cat, count in category_counter.most_common():
        report.append(f"- {cat}: {count}æ¡")
    report.append(f"")
    
    source_counter = Counter([x['source'] for x in items_sorted])
    report.append(f"**æ•°æ®æºåˆ†å¸ƒ**ï¼š")
    for source, count in source_counter.most_common():
        report.append(f"- {source}: {count}æ¡")
    report.append(f"")
    
    # ä¸‹ä¸€æ­¥å»ºè®®
    report.append(f"---")
    report.append(f"")
    report.append(f"## ğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®")
    report.append(f"")
    
    if high_heat:
        report.append(f"âœ… å»ºè®®ç«‹å³å¯¹ä»¥ä¸‹é«˜çƒ­åº¦è¯é¢˜è¿›è¡Œæ·±åº¦è°ƒç ”ï¼š")
        for item in high_heat[:3]:
            report.append(f"- {item['title']}")
        report.append(f"")
        report.append(f"å¯ä½¿ç”¨ `python topic_scorer.py \"ä¸»é¢˜åç§°\"` è¿›è¡Œè¯„åˆ†")
    else:
        report.append(f"âš ï¸  ä»Šæ—¥æ— é«˜çƒ­åº¦è¯é¢˜ï¼Œå»ºè®®ï¼š")
        report.append(f"- è§‚å¯Ÿä¸­çƒ­åº¦è¯é¢˜çš„å‘å±•")
        report.append(f"- æˆ–ä»é€‰é¢˜åº“ä¸­é€‰æ‹©å¸¸é’é€‰é¢˜")
    
    return "\n".join(report)

# ============================================
# ä¸»ç¨‹åº
# ============================================

def run_monitor(test_mode=False):
    """è¿è¡Œç›‘æ§"""
    print(f"\n{'='*60}")
    print(f"ğŸ¤– AIçƒ­ç‚¹ç›‘æ§ç³»ç»Ÿ")
    print(f"{'='*60}\n")
    
    all_items = []
    
    # é‡‡é›†æ•°æ®
    if test_mode:
        print("âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šä»…æŠ“å–HackerNews\n")
        all_items.extend(fetch_hackernews())
    else:
        all_items.extend(fetch_hackernews())
        all_items.extend(fetch_brave_search())
        all_items.extend(fetch_producthunt())
        all_items.extend(fetch_arxiv())
        all_items.extend(fetch_techcrunch())
        all_items.extend(fetch_github_trending())
    
    # å»é‡
    all_items = deduplicate(all_items)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æ•°æ®é‡‡é›†å®Œæˆï¼šå…± {len(all_items)} æ¡ï¼ˆå»é‡åï¼‰")
    print(f"{'='*60}\n")
    
    if len(all_items) == 0:
        print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•çƒ­ç‚¹ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å…³é”®è¯é…ç½®")
        return None, None
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_report(all_items)
    
    # ä¿å­˜ï¼ˆåŒæ—¶ç”Ÿæˆå›ºå®šæ–‡ä»¶åå’Œå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½ï¼‰
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # å›ºå®šæ–‡ä»¶åï¼ˆä¾›å®šæ—¶ä»»åŠ¡ä½¿ç”¨ï¼‰
    fixed_report_file = "topic_monitor_report.md"
    with open(fixed_report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆï¼š{fixed_report_file}")
    
    # å¸¦æ—¶é—´æˆ³çš„å¤‡ä»½
    backup_report_file = f"ai_trending_{timestamp}.md"
    with open(backup_report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"ğŸ“¦ å¤‡ä»½å·²ä¿å­˜ï¼š{backup_report_file}\n")
    
    # ä¿å­˜JSONï¼ˆåŒæ ·ä¿å­˜ä¸¤ä»½ï¼‰
    fixed_json_file = "topic_monitor_report.json"
    with open(fixed_json_file, 'w', encoding='utf-8') as f:
        json.dump(all_items, f, ensure_ascii=False, indent=2)
    
    backup_json_file = f"ai_trending_{timestamp}.json"
    with open(backup_json_file, 'w', encoding='utf-8') as f:
        json.dump(all_items, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ•°æ®å·²ä¿å­˜ï¼š{fixed_json_file}")
    print(f"ğŸ“¦ å¤‡ä»½å·²ä¿å­˜ï¼š{backup_json_file}\n")
    
    return fixed_report_file, fixed_json_file

def main():
    """ä¸»å‡½æ•°"""
    test_mode = "--test" in sys.argv
    daemon_mode = "--daemon" in sys.argv
    
    if daemon_mode:
        print("ğŸ”„ åå°è¿è¡Œæ¨¡å¼ï¼ˆæ¯4å°æ—¶æ‰§è¡Œä¸€æ¬¡ï¼‰")
        print("æŒ‰ Ctrl+C åœæ­¢\n")
        
        while True:
            try:
                run_monitor(test_mode)
                next_run = datetime.now() + timedelta(hours=4)
                print(f"\nâ° ä¸‹æ¬¡è¿è¡Œæ—¶é—´ï¼š{next_run.strftime('%Y-%m-%d %H:%M:%S')}")
                print(f"â³ ç­‰å¾…ä¸­...\n")
                time.sleep(4 * 3600)
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç›‘æ§å·²åœæ­¢")
                break
            except Exception as e:
                print(f"\nâŒ è¿è¡Œå‡ºé”™ï¼š{e}")
                print(f"â° 5åˆ†é’Ÿåé‡è¯•...\n")
                time.sleep(300)
    else:
        run_monitor(test_mode)

if __name__ == "__main__":
    main()
